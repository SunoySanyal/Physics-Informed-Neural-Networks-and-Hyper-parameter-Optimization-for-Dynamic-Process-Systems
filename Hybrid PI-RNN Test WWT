
import pandas as pd
import requests
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import RNN, Dense, Layer
from tensorflow.keras import Sequential
from tensorflow.python.framework import tensor_shape
from tensorflow import float32, concat, convert_to_tensor
from sklearn import metrics

class EulerIntegratorCell(Layer):
    def __init__(self, ixb, YA, dKlayer, Snheff0=None, units=1, **kwargs):
        super(EulerIntegratorCell, self).__init__(**kwargs)
        self.units = units
        self.ixb     = ixb
        self.YA     = YA
        self.Snheff0    = Snheff0
        self.dKlayer     = dKlayer
        self.state_size  = tensor_shape.TensorShape(self.units)
        self.output_size = tensor_shape.TensorShape(self.units)

    def build(self, input_shape, **kwargs):
        self.built = True

    def call(self, inputs, states):
        inputs=convert_to_tensor(inputs)
        print(inputs.shape)
        Snheff_tm1=convert_to_tensor(states)
        print(Snheff_tm1.shape)
        S_d_tm1 = concat((inputs[:,8,None], Snheff_tm1[0, :]), axis=1)
        print(S_d_tm1.shape)
        dk_t    = self.dKlayer(S_d_tm1)
        dSnheff_t    = (self.ixb + 1/self.YA) * (dk_t) 
        Snheff       = dSnheff_t + Snheff_tm1[0, :]
        return Snheff, [Snheff]
    
    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
        return self.Snheff0
    
class Normalization(Layer):
    def __init__(self, S_low, S_up, a_low, a_up, **kwargs):
        super(Normalization, self).__init__(**kwargs)
        self.low_bound_S   = S_low
        self.upper_bound_S = S_up
        self.low_bound_a   = a_low
        self.upper_bound_a = a_up

    def build(self, input_shape, **kwargs):
        self.built = True

    def call(self, inputs):
        output  = (inputs - [self.low_bound_S, self.low_bound_a]) / [(self.upper_bound_S - self.low_bound_S), (self.upper_bound_a - self.low_bound_a)]
        return output

def create_model(ixb, YA, Snheff0, dKlayer, batch_input_shape, return_sequences=False, return_state=False):
    euler = EulerIntegratorCell(ixb=ixb, YA=YA, dKlayer=dKlayer, Snheff0=Snheff0, batch_input_shape=batch_input_shape)
    PINN  = RNN(cell=euler, batch_input_shape=batch_input_shape, return_sequences=return_sequences, return_state=return_state)
    model = Sequential()
    model.add(PINN)
    model.compile(loss='mse', optimizer='Adam')
    return model


if __name__ == "__main__":
    #coefficients
    [ixb, YA] = [0.08, 0.24]
    
    url='https://github.com/TuseAsrav/Physics-Informed-Neural-Networks-and-Hyper-parameter-Optimization-for-Dynamic-Process-Systems/blob/main/rainyweather5.xlsx?raw=true'
    myfile = requests.get(url)

    df=pd.read_excel(myfile.content)
    dff=df.drop(['t','Xbh5d1000','So5','Xs5','Xi5','Xp5','Xba5','Ss5','Xbh5','Sno5','Snd5','Salk5','ro1','ro2','ro3','ro6','ro3Eksik','carp','deltaSnh5'],axis=1)
    dfff=dff.iloc[674:924]
    data=np.array(dfff)
    
    carpÄ±m=df.iloc[679:824,-3].values
    
    n=data.shape[0]
    d=data.shape[1]
    
    normalized_data=np.zeros((n,d))
    
    for i in range(n):
      for j in range(d):
        normalized_data[i,j]=2*((data[i,j]-np.min(data[:,j]))/(np.max(data[:,j])-np.min(data[:,j])))-1
    
    new_set= pd.DataFrame(normalized_data, columns = dfff.columns)
    
    training_set=new_set.iloc[0:150]
    test_set=new_set.iloc[150:]

    def backnorm(x): 
        backnormOutput=(x+1)/2*(np.max(data[:,-1]-np.min(data[:,-1])))+np.min(data[:,-1])
    
        return backnormOutput
    
    def create_data_sequence(X, y, time_steps=1):
    
        input_sequence, output = [], []
        for i in range(len(X) - time_steps):
            sequence = X.iloc[i:(i + time_steps)].values
            input_sequence.append(sequence)        
            output.append(y.iloc[i + time_steps])
        return np.array(input_sequence), np.array(output)
    
    time_steps = 5
    
    training_set_sequence, training_set_output = create_data_sequence(training_set, training_set.Snh5, time_steps)
    test_set_sequence, test_set_output = create_data_sequence(test_set, test_set.Snh5, time_steps)
    Snheff0= np.asarray(15*np.ones((test_set_sequence.shape[0],1)))
        
    dKlayer = Sequential()
    dKlayer.add(Normalization(np.min(training_set_sequence[:,:,8,None]), np.max(training_set_sequence[:,:,8,None]), np.min(training_set_output), np.max(training_set_output)))
    dKlayer.add(Dense(25, activation='tanh'))
    dKlayer.add(Dense(25, activation='tanh'))
    dKlayer.add(Dense(1))

    Input_range  = np.linspace(np.min(test_set_sequence[:,:,8,None]), np.max(test_set_sequence[:,:,8,None]), 1000)
    Output_range  = np.linspace(np.min(test_set_output), np.max(test_set_output), 1000)[np.random.permutation(np.arange(1000))]
    dK_range = 0.9*Input_range + Output_range*0.2
    
    dKlayer.compile(loss='mse', optimizer='Adam')
    inputs_train = np.transpose(np.asarray([Input_range, Output_range]))
    dKlayer.fit(inputs_train, dK_range, epochs=200)

    model = create_model(ixb=ixb, YA=YA, Snheff0=convert_to_tensor(Snheff0, dtype=float32), dKlayer=dKlayer, batch_input_shape=test_set_sequence.shape)
    
     # loading weights from trained model
    model.load_weights("./savedmodels/cp.ckpt")
    Snheff = model.predict_on_batch(test_set_sequence)
    Snh=backnorm(Snheff)
    actual_testset_values=backnorm(test_set_output)
    
    mse=metrics.mean_squared_error(actual_testset_values, Snh[:,-1])
    print('RNN:Test MSE: ', mse)
        
    fig,ax = plt.subplots()
    x=df['t']
    plt.plot(x[824:919,], Snh[:,-1], label='Predicted Test Snh5')
    plt.plot(x[824:919,], actual_testset_values, label='Actual Test Snh5')
    ax.set_xlabel('Days')
    ax.set_ylabel('Snh5')
    plt.legend();
    plt.show() 
