import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import RNN, Dense, Layer, LSTM, SimpleRNN
from tensorflow.keras import Sequential
from tensorflow.python.framework import tensor_shape
from tensorflow import float32, concat, convert_to_tensor
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

class EulerIntegratorCell(Layer):
    def __init__(self, k,v0, dKlayer, CC0=None, units=1, **kwargs):
        super(EulerIntegratorCell, self).__init__(**kwargs)
        self.units = units
        self.k     = k
        self.v0    =v0
        self.CC0    =CC0
        self.dKlayer=dKlayer
        self.state_size  = tensor_shape.TensorShape(self.units)
        self.output_size = tensor_shape.TensorShape(self.units)

    def build(self, input_shape, **kwargs):
        self.built = True

    def call(self, inputs, states):
        inputs  = convert_to_tensor(inputs)
        CC_tm1   = convert_to_tensor(states)
        x_d_tm1 = concat((inputs[:,2,None], CC_tm1[0, :]), axis=1)
        dk_t    = self.dKlayer(x_d_tm1)
        dCC_t    = self.k*inputs[:,0,None]*inputs[:,1,None]-dk_t
        CC       = dCC_t + CC_tm1[0, :]
        return CC, [CC]

class Normalization(Layer):
    def __init__(self, C_low, C_up, CC_low, CC_up, **kwargs):
        super(Normalization, self).__init__(**kwargs)
        self.low_bound_C  = C_low
        self.upper_bound_C = C_up
        self.low_bound_CC   = CC_low
        self.upper_bound_CC = CC_up

    def build(self, input_shape, **kwargs):
        self.built = True

    def call(self, inputs):
        output  = (inputs - [self.low_bound_C, self.low_bound_CC]) / [(self.upper_bound_C - self.low_bound_C), (self.upper_bound_CC - self.low_bound_CC)]
        return output

def create_model(k, v0, CC0, dKlayer, batch_input_shape, return_sequences=False, return_state=False):
    euler = EulerIntegratorCell(k=k, v0=v0, CC0=CC0, dKlayer=dKlayer, batch_input_shape=batch_input_shape)
    PINN  = RNN(cell=euler, batch_input_shape=batch_input_shape, return_sequences=return_sequences, return_state=return_state)
    model = Sequential()
    model.add(PINN)
    model.compile(loss='mse', optimizer='Adam')
    return model

if __name__ == "__main__":
    #coefficients
    [k,v0,V0] = [0.2, 3, 100]
    
    df=pd.read_excel(r'C:\Users\Tuse\Desktop\semibatch2.xlsx')
    dff=df.drop(['time','dk','deltaCC'],axis=1)
    data=np.array(dff)
    
    n=data.shape[0]
    d=data.shape[1]
    
    normalized_data=np.zeros((n,d))
    
    for i in range(n):
      for j in range(d):
        normalized_data[i,j]=2*((data[i,j]-np.min(data[:,j]))/(np.max(data[:,j])-np.min(data[:,j])))-1
    
    new_set= pd.DataFrame(normalized_data, columns = dff.columns)
    
    x=new_set.iloc[:,0:3]
    y=new_set.iloc[:,2:3]
    
    training_set, test_set, training_output, test_output=train_test_split(x,y, test_size=0.7, shuffle=False)

    def backnormC(x): 
      backnormCC=(x+1)/2*(np.max(data[:,2]-np.min(data[:,2])))+np.min(data[:,2])
    
      return backnormCC

    def create_data_sequence(X, y, time_steps=1):
    
        input_sequence, output= [], []
        for i in range(len(X) - time_steps):
            sequence = X.iloc[i:(i + time_steps)].values
            input_sequence.append(sequence)        
            output.append(y.iloc[i + time_steps])
            
        return np.array(input_sequence), np.array(output)
    
    time_steps = 5
    
    training_set_sequence, training_set_output = create_data_sequence(training_set, training_set.CC, time_steps)
    test_set_sequence, test_set_output = create_data_sequence(test_set, test_set.CC, time_steps)

    CC0= np.asarray(0.23*np.ones((test_set_sequence.shape[0],1)))

    dKlayer = Sequential()
    dKlayer.add(Dense(25, activation='tanh'))
    dKlayer.add(Dense(25, activation='tanh'))
    dKlayer.add(Dense(1))

    # weight initialization
    C_range  = np.linspace(np.min(test_set_sequence[:,:,2,None]), np.max(test_set_sequence[:,:,2,None]), 1000)
    CC_range  = np.linspace(np.min(test_set_output), np.max(training_set_output), 1000)[np.random.permutation(np.arange(1000))]
    dK_range = 0.1*C_range + CC_range

    dKlayer.compile(loss='mse', optimizer='Adam')
    inputs_train = np.transpose(np.asarray([C_range, CC_range]))
    dKlayer.fit(inputs_train, dK_range, epochs=200)
    
    model = create_model(k=k, v0=v0, CC0=convert_to_tensor(CC0, dtype=float32), dKlayer=dKlayer, batch_input_shape=test_set_sequence.shape)
    
    model.load_weights("./savedmodels/cp.ckpt")
    CC = model.predict_on_batch(test_set_sequence)
    CCPred=backnormC(CC)
    actual_testset_values=backnormC(test_set_output)
    
    mse=metrics.mean_squared_error(actual_testset_values, CCPred[:,-1])
    print('RNN:Test MSE: ', mse)
    
    fig,ax = plt.subplots()
    x=df['time']
    plt.plot(x[150:-5,], CCPred[:,-1], label='Predicted Test CC')
    plt.plot(x[150:-5,], actual_testset_values, label='Actual Test CC')
    ax.set_xlabel('Minutes')
    ax.set_ylabel('CC')
    plt.legend();
    plt.show() 
